Q2_SortAnalysis.txt

Part A:

Methodology
Tests are done by a shell script that runs various sorts on ascending, descending and random data sets.  
Sample size 0, thorugh 8192, where each result is the average of 4 runs for each sort type.  
Time is in nanoseconds, rounded down to 1dp.  

Big-O notation is intended to describe how an algorithm scales for variable size input.
(the fastest possible sorting time for a set is O(n)).

:Results taken from P01/results.txt and P01/findings.md
Insertion Sort					
Data Size	|	Best Case	|	Worst Case	|	Average Case
1		|	0.0		| 	0.0		| 	0.0
2 		|  	0.0		| 	0.0		| 	0.3
4		|	0.0 		| 	0.0		| 	0.3
8		|	0.0		| 	10.0		|	1.0
16		|	1.0		| 	25.3		| 	3.0
32		|	1.0		| 	74.3		| 	13.6
64		|	4.3		| 	256.0		| 	40.6
128		|	9.6		| 	271.6		| 	126.3
256		|	3.0		| 	239.0		| 	856.6
512		| 	2.0		|	469.0		| 	478.3
1024		|	4.0		| 	1426.0		| 	233.6
4096		|	11.0		| 	5853.3		|	4737.6
8192		|	26.0		| 	27445.3		| 	12287.0
Discussion :
Insertion sort is known as one of the three 'in-place' sorting algorithms.
Its time complexity is O(n^2) due to its use of nested for loops.

Insertion sort has characteristics such as having a 'sorted' and 'unsorted' part of the array.
The values from the unsorted part and picked and placed in the correct position in the sorted part.
This may explain why using sorted ascending data is much more efficient that using sorted descending data.

    Notes:
Best case -> sorted ascending data
Worst case -> sorted descending data
Average case -> unsorted random data


:Results taken from P11/script_results.txt and P11/finding2.md
ShellSort	
Data Size	|	Best Case	|	Worst Case	|	Average Case
1		|	0.0		| 	0.3		|	0.0
2 		|	0.0		|	1.0		|	0.0
4		|	0.0		|	0.0		|	0.0
8		|	0.6		|	1.0		|	1.0
16		|	2.0 		|	3.0		|	3.0
32		|	5.6		|	9.3		|	9.0
64		|	14.0		|	28.0		|	21.3
128		|	33.0		|	72.0		|	51.3
256		|	74.6		| 	169.3		|	117.6
512		|	183.6		|	430.3		|	590.6
1024		|	382.6		|	1146.6		|	81.3
4096		|	118.0		|	408.3		|	179.0
8192		|	257.0		|	2195.6		|	391.6
Discussion :
Shellsort has a time complexity of O(n Log n). We would hypothesize that on average it is faster
than O(n^2) algorithms. This proposal is evidently true by comparing the average case of Shellsort against
Insertion sort (where n = 8192). The results are 391.6ns (shellsort) vs 12287.0ns (insertions sort)
Evidently Shellsort is much faster than insertion sort.

The reason it is much faster is due to good use of recursion.
eg: shellsort is O(n Log n), as we copy N elements at each level in recursion.
And comparisons can in each level will be less than N. (splitting by logN).
(Recurisve splitting is natural log of N)

Recursive functions can cause increased space complexity on the stack,
and are limited by the stack size. After the sorting the stacks are deallocated from memory

    Notes:
Best case -> sorted ascending data
Worst case -> unsorted random data
Average case -> sorted descending data


:Results taken from P11/script_results.txt and P11/finding2.md
QuickSort
Strategy 1 - Right Most
Data Size	|	Best Case	|	Worst Case	|	Average Case
1		|	71.6		|	83.3		|	56.6	
2 		|	98.0		|	89.0		|	93.0
4		|	209.0		|	152.3		|	262.6
8		|	274.0		|	263.3		| 	957.6
16		|	506.3		|	459.3		| 	1788.3
32		|	1520.3		|	846.0		| 	2684.3
64		|	2683.3		|	1503.3		| 	3132.3
128		|	3798.3		|	2130.3		| 	7318.3
256		|	6099.6		|	3064.0		| 	13902.6
512		|	9042.0		|	11266.6		| 	25173.0
1024		|	12985.3		|	11613.3		| 	47824.3
4096		|	44525.6		|	49996.3		| 	47924.3
8192		|	84993.9		|	117494.3	| 	109002.6
    Notes:
Best case -> unsorted random data
Worst case -> sorted descending data
Average case -> sorted ascending data


Strategy 2 - Median of Three
Data Size	|	Best Case	|	Worst Case	|	Average Case
1		|	54.0		| 	63.6		| 	54.0
2 		|	118.6		|	105.3		|	133.6
4		|	151.3		|	157.3		|	149.0
8		|	459.3		|	597.0		|	468.6
16		|	711.3		|	948.0		|	415.3
32		|	798.3		|	1356.6		|	818.0
64		|	1576.3		|	1796.0		|	1570.3
128		|	2951.6		|	3021.3		|	2929.3
256		|	6428.0		|	7299.0		|	6882.3
512		|	10616.6		|	12018.3		|	10726.3
1024		|	20922.6		|	20470.3		|	19929.6
4096		|	37015.0		|	38449.0		|	36723.3
8192		|	73713.6		|	83948.3		|	73646.33
Discussion:
Quicksort is a 'divide and conquer' algorithm. QuickSorts variants are also O(n log n)

Results show that quicksort may may be slower than O(n^2) algorithms such as insertion sort.
This is due to insertsion sort and quicksort sharing a worst case complextity of O(n^2).

A key note about quicksort is that in-place partitioning can be unstable. (may swap elements of the same weight
which can lead to data errors).

Quicksort uses partitioning - notably using recursion, where each partition calls a function
to operate on an array size [i(current element position) - 1].

Median of Three is faster than the using a Right Most partitioning approach. This may be 
due to the rightmost approach creating much unbalanced partition which can degrade the algorithm.

    Notes:
Best case -> sorted descending data
Worst case -> unsorted random data
Average case -> sorted ascending data


Part B
In a text file Q2discuss.txt, reflect on the trees from b) in terms of:

I. the  heights  of  the  resultant  trees  â€“  how  do  they  compare  for  the  same  input values?
    Binary Search Tree had the largest height of (8), followed by Red Black Tree (5), then
    2-3-4 tree (3) and lastly B-Tree 6key per-node (2)

    The heights of the resultant trees are smaller than the height of the binary search
    trees. Due to the fact a binary search tree is NOT self-balancing - there may
    be certain instances where the tree is more left/right dominant. This leads to
    increase hight of the tree. Red-Black, 2-3-4 and B trees are all self-balancing,
    meaning nodes may split, expand and replace each other. This means the trees
    can on average, balance to a height height of (log n). Note that binary search trees
    can be (log n) height, but falter in worst-case scenarios (ascending/descending data)
	

II.   Compare the understandability of the algorithms, which would be easier to implement?
    2-3-4 Trees and Red-Black trees are equivalent data structures, how
    ever 2-3-4 trees can involve more complex node splits and expansions
    compared to Red-Blacks Trees, making them harder to implement
    B trees are not as complex as 2-3-4 trees but not as simple as redblack trees. 
    They are much simpler in terms of traversal as keys are
    stored in a sequential sorted order. This is especially useful for storing
    database and hard-drive data.


